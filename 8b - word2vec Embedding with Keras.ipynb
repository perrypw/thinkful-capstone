{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many thanks to Jason Brownlee's [blog post](https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/) on this topic, which was extremely helpful in getting this up and running considering I have not had a chance to do the Keras specialization yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.externals import joblib\n",
    "from sklearn import metrics\n",
    "shows = pd.read_pickle(\"no_na_pre2017_v4.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('/Users/perrypetra-wong/Dropbox/Thinkful/Lessons/Capstone/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "#model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our text documents, let's start with a concatenation of the three text columns, and define our labels: cancellation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A sitcom based on the Twitter feed \"S*** My Dad Says\", which was created by Justin Halpern and is filled with quotes said by his father.Ed is an opinionated and divorced 72-year-old man. His two sons - Henry and Vince - are both adults and over the years have become very accustomed to his unsolicited rants, which are often politically incorrect.When Henry, a struggling writer who also blogs, can\\'t afford to pay his rent any longer, he\\'s forced to move back in with his dad, which creates more issues in their already tricky father-son relationship.During one of Henry\\'s job interviews, Ed interrupts with one of his usual irrational phone calls. This catches the ear of the interviewer, who ends up hiring Henry, but also forces him to remain living with his dad so he can keep writing about his rantings.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = shows['tagline'].astype('O') + shows['synopsis'].astype('O')\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = shows['two_season_cancel']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limiting our word vectors to just those present in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "embeddings_index = {}\n",
    "\n",
    "for wordstring in docs:\n",
    "    \n",
    "    if isinstance(wordstring,float):\n",
    "        pass\n",
    "    else:\n",
    "        # In case of null\n",
    "        punct_remover = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "        # Removing punctuation and splitting apart words\n",
    "        punct_stripped = wordstring.translate(punct_remover)\n",
    "        wordlist = punct_stripped.split()\n",
    "\n",
    "        # If word is in Google model, add it its corresponding vec to our running\n",
    "        # subset dict\n",
    "        for word in wordlist:\n",
    "            word = word.lower()\n",
    "            if word in model:\n",
    "                if word not in embeddings_index:\n",
    "                    embeddings_index[word] = model[word]\n",
    "                    \n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#joblib.dump(embeddings_index,\"Extras/embeddings_index.pkl\")\n",
    "embeddings_index = joblib.load(\"Extras/embeddings_index.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the texts to save mappings from words to integers\n",
    "t = keras.preprocessing.text.Tokenizer()\n",
    "t.fit_on_texts(docs)\n",
    "vocab_size = len(t.word_index) + 1\n",
    "\n",
    "encoded_docs = t.texts_to_sequences(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 253, 80, 14, 1, 2994, 2995, 346, 1356, 390, 3867, 83, 59, 659, 26, 1357, 5648, 4, 7, 1218, 8, 5649, 2046, 26, 9, 146, 2996, 7, 18, 3868, 4, 336, 3869, 127, 135, 96, 9, 48, 1130, 660, 4, 2997, 21, 155, 907, 4, 85, 1, 68, 53, 148, 491, 5650, 5, 9, 3870, 5651, 83, 21, 406, 2998, 2999, 36, 660, 2, 289, 908, 10, 79, 5652, 1544, 5653, 5, 2420, 9, 5654, 347, 1545, 492, 262, 5, 198, 70, 6, 8, 9, 390, 83, 2421, 93, 247, 6, 12, 1219, 3000, 146, 139, 212, 185, 41, 3, 5655, 128, 1045, 2996, 5656, 8, 41, 3, 9, 5657, 5658, 3001, 1546, 27, 3002, 1, 5659, 3, 1, 5660, 10, 379, 39, 5661, 660, 35, 79, 493, 62, 5, 1131, 105, 8, 9, 390, 254, 24, 92, 255, 2047, 75, 9, 5662]]\n"
     ]
    }
   ],
   "source": [
    "print(encoded_docs[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to pad the length of these to a fixed amount that won't be exceeded\n",
    "max_length = 25\n",
    "padded_docs = keras.preprocessing.sequence.pad_sequences(encoded_docs, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to create a matrix of one embedding per word in the training set. We can do that by enumerating all unique words in the Tokenizer.word_index and locating the corresponding vector in the word2vec subset we've created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight matrix for each word in training doc\n",
    "embedding_matrix = np.zeros((vocab_size, 300))\n",
    "for word, i in t.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 25, 300)           3176100   \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 7500)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 7501      \n",
      "=================================================================\n",
      "Total params: 3,183,601\n",
      "Trainable params: 7,501\n",
      "Non-trainable params: 3,176,100\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "e = keras.layers.Embedding(vocab_size,300,weights=[embedding_matrix],input_length=max_length,trainable=False)\n",
    "\n",
    "# Define the model\n",
    "model = keras.Sequential()\n",
    "model.add(e)\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(1,activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1316/1316 [==============================] - 0s 162us/step - loss: 0.6902 - acc: 0.5585\n",
      "Epoch 2/50\n",
      "1316/1316 [==============================] - 0s 34us/step - loss: 0.6028 - acc: 0.7264\n",
      "Epoch 3/50\n",
      "1316/1316 [==============================] - 0s 38us/step - loss: 0.5491 - acc: 0.7903\n",
      "Epoch 4/50\n",
      "1316/1316 [==============================] - 0s 36us/step - loss: 0.5018 - acc: 0.8625\n",
      "Epoch 5/50\n",
      "1316/1316 [==============================] - 0s 35us/step - loss: 0.4642 - acc: 0.8868\n",
      "Epoch 6/50\n",
      "1316/1316 [==============================] - 0s 36us/step - loss: 0.4294 - acc: 0.9149\n",
      "Epoch 7/50\n",
      "1316/1316 [==============================] - 0s 38us/step - loss: 0.3986 - acc: 0.9392\n",
      "Epoch 8/50\n",
      "1316/1316 [==============================] - 0s 38us/step - loss: 0.3745 - acc: 0.9422\n",
      "Epoch 9/50\n",
      "1316/1316 [==============================] - 0s 37us/step - loss: 0.3493 - acc: 0.9559\n",
      "Epoch 10/50\n",
      "1316/1316 [==============================] - 0s 36us/step - loss: 0.3280 - acc: 0.9597\n",
      "Epoch 11/50\n",
      "1316/1316 [==============================] - 0s 42us/step - loss: 0.3071 - acc: 0.9726\n",
      "Epoch 12/50\n",
      "1316/1316 [==============================] - 0s 45us/step - loss: 0.2889 - acc: 0.9787\n",
      "Epoch 13/50\n",
      "1316/1316 [==============================] - 0s 52us/step - loss: 0.2722 - acc: 0.9833\n",
      "Epoch 14/50\n",
      "1316/1316 [==============================] - 0s 42us/step - loss: 0.2570 - acc: 0.9871\n",
      "Epoch 15/50\n",
      "1316/1316 [==============================] - 0s 40us/step - loss: 0.2440 - acc: 0.9886\n",
      "Epoch 16/50\n",
      "1316/1316 [==============================] - 0s 44us/step - loss: 0.2309 - acc: 0.9909\n",
      "Epoch 17/50\n",
      "1316/1316 [==============================] - 0s 44us/step - loss: 0.2182 - acc: 0.9932\n",
      "Epoch 18/50\n",
      "1316/1316 [==============================] - 0s 44us/step - loss: 0.2075 - acc: 0.9970\n",
      "Epoch 19/50\n",
      "1316/1316 [==============================] - 0s 41us/step - loss: 0.1967 - acc: 0.9954\n",
      "Epoch 20/50\n",
      "1316/1316 [==============================] - 0s 39us/step - loss: 0.1865 - acc: 0.9970\n",
      "Epoch 21/50\n",
      "1316/1316 [==============================] - 0s 41us/step - loss: 0.1784 - acc: 0.9992\n",
      "Epoch 22/50\n",
      "1316/1316 [==============================] - 0s 41us/step - loss: 0.1693 - acc: 0.9992\n",
      "Epoch 23/50\n",
      "1316/1316 [==============================] - 0s 42us/step - loss: 0.1614 - acc: 0.9992\n",
      "Epoch 24/50\n",
      "1316/1316 [==============================] - 0s 42us/step - loss: 0.1543 - acc: 0.9992\n",
      "Epoch 25/50\n",
      "1316/1316 [==============================] - 0s 41us/step - loss: 0.1477 - acc: 0.9992\n",
      "Epoch 26/50\n",
      "1316/1316 [==============================] - 0s 42us/step - loss: 0.1412 - acc: 0.9992\n",
      "Epoch 27/50\n",
      "1316/1316 [==============================] - 0s 44us/step - loss: 0.1348 - acc: 0.9992\n",
      "Epoch 28/50\n",
      "1316/1316 [==============================] - 0s 45us/step - loss: 0.1291 - acc: 0.9992\n",
      "Epoch 29/50\n",
      "1316/1316 [==============================] - 0s 39us/step - loss: 0.1234 - acc: 0.9992\n",
      "Epoch 30/50\n",
      "1316/1316 [==============================] - 0s 44us/step - loss: 0.1185 - acc: 0.9992\n",
      "Epoch 31/50\n",
      "1316/1316 [==============================] - 0s 44us/step - loss: 0.1136 - acc: 0.9992\n",
      "Epoch 32/50\n",
      "1316/1316 [==============================] - 0s 45us/step - loss: 0.1093 - acc: 0.9992\n",
      "Epoch 33/50\n",
      "1316/1316 [==============================] - 0s 37us/step - loss: 0.1048 - acc: 0.9992\n",
      "Epoch 34/50\n",
      "1316/1316 [==============================] - 0s 43us/step - loss: 0.1010 - acc: 0.9992\n",
      "Epoch 35/50\n",
      "1316/1316 [==============================] - 0s 44us/step - loss: 0.0971 - acc: 0.9992\n",
      "Epoch 36/50\n",
      "1316/1316 [==============================] - 0s 47us/step - loss: 0.0933 - acc: 0.9992\n",
      "Epoch 37/50\n",
      "1316/1316 [==============================] - 0s 45us/step - loss: 0.0900 - acc: 0.9992\n",
      "Epoch 38/50\n",
      "1316/1316 [==============================] - 0s 44us/step - loss: 0.0865 - acc: 0.9992\n",
      "Epoch 39/50\n",
      "1316/1316 [==============================] - 0s 38us/step - loss: 0.0834 - acc: 0.9992\n",
      "Epoch 40/50\n",
      "1316/1316 [==============================] - 0s 38us/step - loss: 0.0805 - acc: 0.9992\n",
      "Epoch 41/50\n",
      "1316/1316 [==============================] - 0s 34us/step - loss: 0.0778 - acc: 0.9992\n",
      "Epoch 42/50\n",
      "1316/1316 [==============================] - 0s 37us/step - loss: 0.0750 - acc: 0.9992\n",
      "Epoch 43/50\n",
      "1316/1316 [==============================] - 0s 36us/step - loss: 0.0725 - acc: 0.9992\n",
      "Epoch 44/50\n",
      "1316/1316 [==============================] - 0s 36us/step - loss: 0.0700 - acc: 0.9985\n",
      "Epoch 45/50\n",
      "1316/1316 [==============================] - 0s 37us/step - loss: 0.0677 - acc: 0.9992\n",
      "Epoch 46/50\n",
      "1316/1316 [==============================] - ETA: 0s - loss: 0.0655 - acc: 0.998 - 0s 41us/step - loss: 0.0655 - acc: 0.9985\n",
      "Epoch 47/50\n",
      "1316/1316 [==============================] - 0s 42us/step - loss: 0.0631 - acc: 0.9992\n",
      "Epoch 48/50\n",
      "1316/1316 [==============================] - 0s 40us/step - loss: 0.0612 - acc: 0.9992\n",
      "Epoch 49/50\n",
      "1316/1316 [==============================] - 0s 39us/step - loss: 0.0594 - acc: 0.9985\n",
      "Epoch 50/50\n",
      "1316/1316 [==============================] - 0s 38us/step - loss: 0.0575 - acc: 0.9992\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x235813f60>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(padded_docs,labels,epochs=50,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(padded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9992401215805471"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(labels,np.round(y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously it performs quite well on the training set, but this was more to just check that the model worked. Let's now run it in a cross-validated fashion in the same way that we've done for our other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "439/439 [==============================] - 0s 86us/step\n",
      "Accuracy on first fold: 56.04%\n",
      "439/439 [==============================] - 0s 106us/step\n",
      "Accuracy on first fold: 53.08%\n",
      "438/438 [==============================] - 0s 120us/step\n",
      "Accuracy on first fold: 54.11%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=3)\n",
    "\n",
    "# reindexing docs and labels to avoid any errors there\n",
    "docs.index = np.arange(0,len(docs))\n",
    "docs = docs.reindex()\n",
    "labels.index = (np.arange(0,len(labels)))\n",
    "\n",
    "for train,test in kf.split(docs):\n",
    "      \n",
    "    # Train and test\n",
    "    docs_train = docs[train]\n",
    "    labels_train = labels[train]\n",
    "    docs_test = docs[test]\n",
    "    labels_test = labels[test]\n",
    "    \n",
    "    # Tokenize the texts to save mappings from words to integers\n",
    "    t = keras.preprocessing.text.Tokenizer()\n",
    "    t.fit_on_texts(docs_train)\n",
    "    vocab_size = len(t.word_index) + 1\n",
    "\n",
    "    encoded_docs = t.texts_to_sequences(docs_train)\n",
    "    \n",
    "    # Capping the lengths\n",
    "    max_length = 25\n",
    "    padded_docs = keras.preprocessing.sequence.pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "    \n",
    "    # Weight matrix for each word in training doc\n",
    "    embedding_matrix = np.zeros((vocab_size, 300))\n",
    "    for word, i in t.word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    \n",
    "    e = keras.layers.Embedding(vocab_size,300,weights=[embedding_matrix],input_length=max_length,trainable=False)\n",
    "    \n",
    "    # Define the model\n",
    "    model = keras.Sequential()\n",
    "    model.add(e)\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dense(1,activation='sigmoid'))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "    \n",
    "    model.fit(padded_docs,labels_train,epochs=50,verbose=0)\n",
    "    \n",
    "    # Test set\n",
    "    encoded_test = t.texts_to_sequences(docs_test)\n",
    "    padded_docs_test = keras.preprocessing.sequence.pad_sequences(encoded_test, maxlen=max_length, padding='post')\n",
    "    \n",
    "    y_pred = model.predict(padded_docs_test)\n",
    "    loss,accuracy = model.evaluate(padded_docs_test,labels_test)\n",
    "    \n",
    "    print('Accuracy on first fold: {0:.2%}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.541095890410959"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confirming how the accuracy seems to be calculated\n",
    "metrics.accuracy_score(np.round(y_pred),labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sadly, we're still looking at pretty poor results. Even though our neural net can easily converge on 100% training accuracy within 50 epochs, accuracy falls apart when applied to holdouts. As such, I don't think adding layers to our net and optimizing would be very fruitful, since we seem to be only hurting our predictive value when we attempt more complex learning from the training documents. In other words, we're observing the bias-variance tradeoff.\n",
    "\n",
    "This is not to say that NLP doesn't have some cool applications in this dataset. Out of curiosity, let's see how this network does when assigned to something else, like telling whether or not a show is a reality show!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = shows['Reality']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "439/439 [==============================] - 0s 364us/step\n",
      "Accuracy on first fold: 78.82%\n",
      "439/439 [==============================] - 0s 366us/step\n",
      "Accuracy on first fold: 81.32%\n",
      "438/438 [==============================] - 0s 407us/step\n",
      "Accuracy on first fold: 80.14%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=3)\n",
    "\n",
    "# reindexing docs and labels to avoid any errors there\n",
    "docs.index = np.arange(0,len(docs))\n",
    "docs = docs.reindex()\n",
    "labels.index = (np.arange(0,len(labels)))\n",
    "\n",
    "for train,test in kf.split(docs):\n",
    "      \n",
    "    # Train and test\n",
    "    docs_train = docs[train]\n",
    "    labels_train = labels[train]\n",
    "    docs_test = docs[test]\n",
    "    labels_test = labels[test]\n",
    "    \n",
    "    # Tokenize the texts to save mappings from words to integers\n",
    "    t = keras.preprocessing.text.Tokenizer()\n",
    "    t.fit_on_texts(docs_train)\n",
    "    vocab_size = len(t.word_index) + 1\n",
    "\n",
    "    encoded_docs = t.texts_to_sequences(docs_train)\n",
    "    \n",
    "    # Capping the lengths\n",
    "    max_length = 35\n",
    "    padded_docs = keras.preprocessing.sequence.pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "    \n",
    "    # Weight matrix for each word in training doc\n",
    "    embedding_matrix = np.zeros((vocab_size, 300))\n",
    "    for word, i in t.word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    \n",
    "    e = keras.layers.Embedding(vocab_size,300,weights=[embedding_matrix],input_length=max_length,trainable=False)\n",
    "    \n",
    "    # Define the model\n",
    "    model = keras.Sequential()\n",
    "    model.add(e)\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dense(30,activation='relu'))\n",
    "    model.add(keras.layers.Dense(1,activation='sigmoid'))\n",
    "\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "    \n",
    "    model.fit(padded_docs,labels_train,epochs=50,verbose=0)\n",
    "    \n",
    "    # Test set\n",
    "    encoded_test = t.texts_to_sequences(docs_test)\n",
    "    padded_docs_test = keras.preprocessing.sequence.pad_sequences(encoded_test, maxlen=max_length, padding='post')\n",
    "    \n",
    "    y_pred = model.predict_classes(padded_docs_test)\n",
    "    loss,accuracy = model.evaluate(padded_docs_test,labels_test)\n",
    "    \n",
    "    print('Accuracy on first fold: {0:.2%}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Drama        0.402736\n",
       "Comedy       0.308511\n",
       "Reality      0.238602\n",
       "Game Show    0.025836\n",
       "Talk         0.022796\n",
       "Sci-fi       0.001520\n",
       "Name: primary_genre, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shows['primary_genre'].value_counts().apply(lambda x:x/len(shows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23860182370820668"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shows['Reality'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty impressive for a quick solution, with 80% accuracy (even if only 5% above baseline). Let's see how it does with all genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "# Need to properly encode the outputs\n",
    "labels = pd.Series(encoder.fit_transform(shows['primary_genre']))\n",
    "encoded_labels = encoder.fit_transform(shows['primary_genre'])\n",
    "one_hot_labels = keras.utils.np_utils.to_categorical(encoded_labels)\n",
    "one_hot_labels = np.array(pd.get_dummies(encoded_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "439/439 [==============================] - 0s 472us/step\n",
      "Accuracy on first fold: 63.55%\n",
      "439/439 [==============================] - 0s 519us/step\n",
      "Accuracy on first fold: 64.69%\n",
      "438/438 [==============================] - 0s 560us/step\n",
      "Accuracy on first fold: 63.24%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=3)\n",
    "\n",
    "# reindexing docs and labels to avoid any errors there\n",
    "docs.index = np.arange(0,len(docs))\n",
    "docs = docs.reindex()\n",
    "\n",
    "for train,test in kf.split(docs):\n",
    "      \n",
    "    # Train and test\n",
    "    docs_train = docs[train]\n",
    "    labels_train = one_hot_labels[train]\n",
    "    docs_test = docs[test]\n",
    "    labels_test = one_hot_labels[test]\n",
    "    \n",
    "    # Tokenize the texts to save mappings from words to integers\n",
    "    t = keras.preprocessing.text.Tokenizer()\n",
    "    t.fit_on_texts(docs_train)\n",
    "    vocab_size = len(t.word_index) + 1\n",
    "\n",
    "    encoded_docs = t.texts_to_sequences(docs_train)\n",
    "    \n",
    "    # Capping the lengths\n",
    "    max_length = 40\n",
    "    padded_docs = keras.preprocessing.sequence.pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "    \n",
    "    # Weight matrix for each word in training doc\n",
    "    embedding_matrix = np.zeros((vocab_size, 300))\n",
    "    for word, i in t.word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    \n",
    "    e = keras.layers.Embedding(vocab_size,300,weights=[embedding_matrix],input_length=max_length,trainable=False)\n",
    "    \n",
    "    # Define the model\n",
    "    model = keras.Sequential()\n",
    "    model.add(e)\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dense(30))\n",
    "    model.add(keras.layers.Dense(30))\n",
    "    model.add(keras.layers.Dense(6,activation='softmax'))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "    \n",
    "    model.fit(padded_docs,labels_train,epochs=50,verbose=0)\n",
    "    \n",
    "    # Test set\n",
    "    encoded_test = t.texts_to_sequences(docs_test)\n",
    "    padded_docs_test = keras.preprocessing.sequence.pad_sequences(encoded_test, maxlen=max_length, padding='post')\n",
    "    \n",
    "    y_pred = model.predict_classes(padded_docs_test)\n",
    "    loss,accuracy = model.evaluate(padded_docs_test,labels_test)\n",
    "    \n",
    "    print('Accuracy on first fold: {0:.2%}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very cool. Almost a 2/3 accuracy vs. a baseline guess of the dominant class, which would have an accuracy of ~40%. Again, this doesn't help our cause of predicting cancellation, which seems like it's just not solvable with NLP given the current information we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_10 (Embedding)     (None, 40, 300)           2551800   \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 12000)             0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 20)                240020    \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 6)                 126       \n",
      "=================================================================\n",
      "Total params: 2,791,946\n",
      "Trainable params: 240,146\n",
      "Non-trainable params: 2,551,800\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
